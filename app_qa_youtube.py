import argparse
from PIL import Image
import streamlit as st 
from langchain.llms import OpenAI
from langchain.chains import RetrievalQA

from transformers import GPT2TokenizerFast
from langchain.text_splitter import TokenTextSplitter
from langchain.document_loaders import YoutubeLoader
from langchain.docstore.document import Document

from langchain.vectorstores import FAISS

from langchain.embeddings import OpenAIEmbeddings
from langchain.embeddings import HuggingFaceEmbeddings



def main():

    st.title("Youtube QA Bot")
    st.header('Header(ì„ì‹œ)')  # ì œëª©ê°™ì€ í° ê¸€ì‹œ
    st.subheader('subheader(ì„ì‹œ)')  # ì œëª©ë³´ë‹¤ëŠ” ì‘ì€ ê¸€ì”¨
    st.text('Youtube ì˜ìƒ ë‹¤ ë³´ê¸°.. ê°€ë” ê·€ì°®ìœ¼ì‹œì£ ? URLë§Œ ë˜ì ¸ì£¼ì„¸ìš”. ê·¸ë¦¬ê³  ë¬¼ì–´ë³´ì„¸ìš”. ë¬´ì—‡ì´ë“  ë‹µí•´ë“œë¦½ë‹ˆë‹¤. ğŸ˜‹') # ì‘ì€ ê¸€ì”¨

    user_in_url = st.text_input(
        "Youtube URLì„ ì…ë ¥í•´ì£¼ì„¸ìš”.",
        placeholder = "https://www.youtube.com/watch?v=-hdlF7UN5KA&t",
    )

    user_in_lang = st.text_input(
        "Youtube ì˜ìƒ ì† ì–¸ì–´ëŠ” ë¬´ì—‡ì¸ì§€ ì•Œë ¤ì£¼ì„¸ìš”(í•œêµ­ì–´ëŠ” koë¼ê³ , ì˜ì–´ëŠ” enì´ë¼ê³  ì…ë ¥í•´ì£¼ì„¸ìš”).",
        placeholder = "ko",
    )

    user_question = st.text_input(
            "ì˜ìƒ ì†ì—ì„œ ê¶ê¸ˆí•œ ë‚´ìš©ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.",
            placeholder = "ì •ì£¼ì˜ ì •ì‹ ì´ë€ ë­ì•¼? 500ì ì´ë‚´ë¡œ ì§§ê²Œ ìš”ì•½í•´ì¤˜."
            )

    # Arguement parsing..
    parser = argparse.ArgumentParser()
    parser.add_argument("--apikey", type=str, required=True, help="If you don't know key value, Just ask jskim")
    parser.add_argument("--model_provider", type=str, default='OpenAI', help="['OpenAI', 'HF']")
    args = parser.parse_args()

    if st.button("Hey ChatGPT. Answer the question right now."):
        ########## API setting ##########
        API=args.apikey
        # If an API key has been provided, create an OpenAI language model instance
        if not API:
            # If an API key hasn't been provided, display a warning message
            st.warning("Enter your OPENAI API-KEY. Get your OpenAI API key from [here](https://platform.openai.com/account/api-keys).\n")


        # 1. get text data from external source(Youtube video transcription)
        # Indexes: Accessing external data
        # Langchainì—ì„œ ì œê³µí•˜ëŠ” Document loaderë¥¼ í†µí•´ ë‹¤ì–‘í•œ sourceì˜ ë°ì´í„°ë¥¼ loadê°€ëŠ¥
        # ì°¸ê³ : https://python.langchain.com/en/latest/modules/indexes/document_loaders.html
        # Youtube video
        # video_url = "https://www.youtube.com/watch?v=-hdlF7UN5KA&t" # korean(HDí˜„ëŒ€)
        # video_url = "https://www.youtube.com/watch?v=SvBR0OGT5VI" # English(TED)
        documents = YoutubeLoader.from_youtube_url(user_in_url, language=user_in_lang).load()


        # 2. text preprocessing(Chunking)
        # Text chunking(Split..)
        tokenizer = GPT2TokenizerFast.from_pretrained("gpt2")
        text_splitter = TokenTextSplitter.from_huggingface_tokenizer(tokenizer,
                                                                     chunk_size=800, 
                                                                     chunk_overlap=200)
        docs = text_splitter.split_text(documents[0].page_content)

        # Text wrapping
        # Langchainì˜ Document class
        # Reference: https://github.com/hwchase17/langchain/blob/master/langchain/text_splitter.py
        # ì‹œê°„ì„ ë§ì´ ì¡ì•„ë¨¹ì€ Debugging point..

        # vectorstoreë¥¼ ë§Œë“œëŠ” FAISS, chroma ëª¨ë‘ .from_documents í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•œë‹¤.
        # ì´ í•¨ìˆ˜ëŠ” ì¸ìë¡œ langchainì˜ Document ê°ì²´ë¥¼ ë°›ëŠ”ë‹¤.
        # ì •í™•íˆëŠ” Documentê°ì²´ì˜ attributeì¸ Document.page_contentë¥¼ ì¸ì‹í•˜ë„ë¡ ì„¤ê³„ë˜ì–´ìˆìŒ.

        # ë¬¸ì œëŠ” ì—¬ê¸°ì„œ ë°œìƒí•¨.
        # ê¸°ë³¸ text_splitter 'CharacterTextSplitter'ëŠ” sepearator='\n\n'ìœ¼ë¡œ ì£¼ì–´ì ¸ìˆì–´
        # ë³¸ ë¶„ì„ ëŒ€ìƒ textì—ì„œ text splitì´ ìˆ˜í–‰ë˜ì§€ ì•ŠìŒ(ë³¸ textëŠ” seperatorë¥¼ íŠ¹ì •í•˜ê¸° ì–´ë ¤ìš´ text)
        # ë”°ë¼ì„œ huggingfaceì˜ tokenizerë¥¼ í™œìš©í•´ tokenë³„ë¡œ splitì„ í•´ì•¼í•¨.

        # í•˜ì§€ë§Œ HF gpt-2 tokenizerì™€ í˜¸í™˜ë˜ëŠ” 'TokenTextSplitter'ëŠ” split_documentsê°€ êµ¬í˜„ë˜ì–´ ìˆì§€ ì•ŠìŒ.
        # ì˜¤ë¡œì§€ split_textë§Œ êµ¬í˜„ë˜ì–´ ìˆìŒ.
        # ë”°ë¼ì„œ 'TokenTextSplitter'ë¡œ splití•˜ë©´ ê·¸ ê²°ê³¼ë¬¼ì€ Documentê°ì²´ê°€ ì•„ë‹Œ textë¡œ ë°˜í™˜ë¨.
        # ë”°ë¼ì„œ 'TokenTextSplitter'ë¡œ splití•œ ê²°ê³¼ë¬¼ì€ vectorstoreì— ì „ë‹¬ë  ìˆ˜ ì—†ìŒ.

        # ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ 'TokenTextSplitter'ë¡œ split ë°˜í™˜ëœ [text1:str, text2:str, ...]ë¥¼
        # langchainì˜ Documentê°ì²´ë¡œ wrappingí•´ì¤Œ
        # [Document1:object, Document2:object, ...]ì™€ ê°™ì´ ë³€í™˜í•´ì¤Œ.

        # ì´ê²Œ ë°”ë¡œ ì•„ë˜ì˜ new_docsë¥¼ ì±„ì›Œê°€ëŠ” ë¼ì¸ì˜ ì—­í• .
        new_docs = [Document(page_content=chunk) for chunk in docs]
        

        # 3. define embedding model & provider
        model_provider='OpenAI'
        if model_provider=='OpenAI':
            embeddings = OpenAIEmbeddings(openai_api_key=API)
        elif model_provider=='HF':
            embeddings = HuggingFaceEmbeddings(model_name = "sentence-transformers/all-MiniLM-L6-v2")


        # 4. create embedding vectorstore(Vector DB) to use as the index
        db = FAISS.from_documents(new_docs, embeddings)


        # 5. Make chin for `question-answering` task with an information retriever
        retriever = db.as_retriever()

        qa = RetrievalQA.from_chain_type(
            llm=OpenAI(openai_api_key=API, max_tokens=500),
            chain_type="stuff", 
            retriever=retriever, 
            return_source_documents=True)

        query = user_question
        result = qa({"query": query})

        st.success(result['result'])


if __name__=='__main__':
    main()